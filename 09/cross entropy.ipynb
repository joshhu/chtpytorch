{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 =  0.35667494393873245\n",
      "loss2 =  2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy example\n",
    "import numpy as np\n",
    "# One hot\n",
    "# 0: 1 0 0 0\n",
    "# 1: 0 1 0 0\n",
    "# 2: 0 0 1 0\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
    "print(\"loss1 = \", np.sum(-Y * np.log(Y_pred1)))\n",
    "print(\"loss2 = \", np.sum(-Y * np.log(Y_pred2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Loss1 =  tensor(0.4170) \n",
      "PyTorch Loss2= tensor(1.8406)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([0]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\n",
    "Y_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"PyTorch Loss1 = \", l1.data, \n",
    "    \"\\nPyTorch Loss2=\", l2.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss1 =  tensor(0.4966) \n",
      "Batch Loss2= tensor(1.2389)\n"
     ]
    }
   ],
   "source": [
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target is of size nBatch\n",
    "# element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.9],\n",
    "                                [1.1, 0.1, 0.2],\n",
    "                                [0.2, 2.1, 0.1]]))\n",
    "\n",
    "Y_pred2 = Variable(torch.Tensor([[0.8, 0.2, 0.3],\n",
    "                                [0.2, 0.3, 0.5],\n",
    "                                [0.2, 0.2, 0.5]]))\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
